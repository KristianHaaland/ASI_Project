{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfd23a9",
   "metadata": {},
   "source": [
    "## Bayesian NN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752a2d4",
   "metadata": {},
   "source": [
    "**NOTE**: This script is designed for use in Kaggle, as it allows the use of GPUs. In order to run the script, download the file and upload it to Kaggle. Then in Kaggle, download the dataset \"mnist-in_csv\" by Dariel Dato-on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb760a1",
   "metadata": {},
   "source": [
    "### Setup explenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17be61e",
   "metadata": {},
   "source": [
    "MNIST has 70000 samples.\n",
    "60000 are used for training, 10000 for testing.\n",
    "10000 samples from training are used for validation, rest for training.\n",
    "\n",
    "The bayesian NN has two layers. \n",
    "First layer has 100 hidden neurons using a sigmoid function.\n",
    "Output layer uses softmax. \n",
    "\n",
    "Optimization based techniques (SGD and SGD with momentum) uses the validation set to select the optimal regularizer lambda of network weights. (Meaning lambda used for the regularization term in the loss function.)\n",
    "\n",
    "For sample based techniques (SGLD and SGHMC), we place a weak gamma prior on each layers weight regularizer lamdba. \n",
    "\n",
    "Sampling using SGLD and SGHMC are done using minibatches of 500 training samples. (In order to compute the gradient).\n",
    "Hyperparameters are resampled after an entire pass over the training set (one epoch).\n",
    "We use a total of 800 iterations/epochs. \n",
    "We have a burn-in of 50 samples.\n",
    "\n",
    "In the Bayesian framework, we are treating lambda as a random variable, and place a prior distribution on it.\n",
    "Since lambda has to be positive, we use a gamma prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ec3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a534cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
    "X_train = train_df.drop('label', axis=1).values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "# Normalize and reshape\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Load test data (also labeled)\n",
    "test_df = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
    "X_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1, 28, 28)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2000, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=784, hidden_dim=100, output_dim=10):\n",
    "        super().__init__() #Calling the constructor of nn.Module\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x) # Apparently cross-entropy loss handles the softmax\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=784, hidden_dim=100, output_dim=10):\n",
    "\n",
    "        super().__init__() #Calling the constructor of nn.Module\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "         # Xavier initialization\n",
    "        #for layer in self.modules():\n",
    "        #    if isinstance(layer, nn.Linear):\n",
    "        #        nn.init.xavier_uniform_(layer.weight)\n",
    "        #        nn.init.zeros_(layer.bias)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)  # logits for softmax\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cf72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_epochs = 100\n",
    "lambda_reg = 1e-04\n",
    "momentum = 0.9\n",
    "init_lr = 0.05\n",
    "\n",
    "def init_training(model, train_loader, test_loader, lr=init_lr, lambda_reg=lambda_reg, momentum=momentum, epochs=init_epochs):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=lambda_reg, momentum=momentum) #L2\n",
    "    \n",
    "    test_errors = []\n",
    "    params = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "    \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        for data, target in train_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        # Evaluate test error after each epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += (pred == target).sum().item()\n",
    "                total += target.size(0)\n",
    "    \n",
    "        test_error = 1 - (correct / total)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        params.append({k: v.clone() for k, v in model.state_dict().items()})\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}, Test Error: {test_error:.4f}\")\n",
    "\n",
    "\n",
    "    return model, test_errors, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87be815",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 700\n",
    "burn_in = 100\n",
    "eps = 0.05 #Learning rate\n",
    "C = 0.1 #Friction\n",
    "a = 1.0\n",
    "b = 10\n",
    "\n",
    "def sghmc_training(model, init_params, train_loader, test_loader, eps=eps, C=C, epochs=epochs, burn_in=burn_in, a=a, b=b):\n",
    "    model.train()\n",
    "    \n",
    "    #for param in model.parameters():\n",
    "    #    nn.init.normal_(param, mean=0.0, std=0.05)\n",
    "\n",
    "    theta = list(model.parameters())\n",
    "    r = [torch.zeros_like(p.data) for p in theta]\n",
    "    \n",
    "    test_errors = []\n",
    "    posteriors = list(init_params)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "            loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            lambda_val = torch.distributions.Gamma(a, b).sample().item()\n",
    "\n",
    "\n",
    "            #---------------------------------------------------------------------\n",
    "\n",
    "            for i, param in enumerate(theta):\n",
    "                \n",
    "                grad_U_tilde = param.grad.data + lambda_val * param.data\n",
    "\n",
    "                #if epoch < burn_in:\n",
    "                #    r[i] = r[i] - eps * grad_U_tilde\n",
    "                #    param.data = param.data + eps * r[i]\n",
    "                #else:\n",
    "                noise = torch.randn_like(param.data) * torch.sqrt(torch.tensor(2 * C * eps))\n",
    "                r[i] = r[i] - eps * grad_U_tilde - eps * C * r[i] + noise\n",
    "                param.data = param.data + eps * r[i]\n",
    "                \n",
    "                #if epoch < burn_in:\n",
    "                #    noise = torch.randn_like(param.data)\n",
    "                #else:\n",
    "                #    noise = torch.randn_like(param.data) * torch.sqrt(torch.tensor(2 * C * eps))\n",
    "                    \n",
    "                #r[i] = r[i] - eps * grad_U_tilde - eps * C * r[i] + noise\n",
    "                #param.data = param.data + eps * r[i]\n",
    "                \n",
    "        #print(f\"Epoch {epoch+1}\")\n",
    "        #for i, param in enumerate(theta):\n",
    "        #    print(f\"Param {i} norm: {param.data.norm():.4f} | r[{i}] norm: {r[i].norm():.4f}\")\n",
    "        #    grad = param.grad.data\n",
    "        #    print(f\"Param {i} grad norm: {grad.norm():.4e}\")\n",
    "\n",
    "\n",
    "            #---------------------------------------------------------------------\n",
    "            \n",
    "        # Evaluate on test set after each epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view(x_test.size(0), -1)\n",
    "                probs = predict(model, posteriors, x_test)\n",
    "                pred = probs.argmax(dim=1)\n",
    "                correct += (pred == y_test).sum().item()\n",
    "                total += y_test.size(0)\n",
    "        test_error = 1 - (correct / total)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        # Save posterior sample\n",
    "        posteriors.append({k: v.clone() for k, v in model.state_dict().items()})\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Test Error: {test_error:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    return posteriors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class of a sample by averaging the output of 10 posterior samples\n",
    "def predict(model, posteriors, sample, n_posteriors=10):\n",
    "    \n",
    "    model.eval()\n",
    "    sample = sample.view(sample.size(0), -1)\n",
    "    pred_probs = torch.zeros((sample.size(0), 10))\n",
    "\n",
    "    chosen_posteriors = random.sample(posteriors, k=n_posteriors)  # Choose 10 sets of posteriors at random\n",
    "\n",
    "    for posterior in chosen_posteriors:\n",
    "        model.load_state_dict(posterior)  # Load model with posterior weights\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(sample)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            pred_probs += probs\n",
    "\n",
    "    return pred_probs / n_posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = SimpleNN()\n",
    "init_model, init_errors, init_params = init_training(init_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianNN()\n",
    "model.load_state_dict(init_params[-1])\n",
    "\n",
    "posteriors, test_errors = sghmc_training(model, init_params, train_loader, test_loader)\n",
    "\n",
    "# Plot test error\n",
    "plt.plot(test_errors)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Test Error\")\n",
    "plt.title(\"Test Error Over Iterations\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7dcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors_final = init_errors + test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'epoch': list(range(1, (init_epochs+epochs) + 1)), 'test_error': test_errors_final})\n",
    "df.to_csv('/kaggle/working/test_error_bayesianNN.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
